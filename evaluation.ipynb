{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0429b1c1",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1869bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Tuple, Optional, Dict, Any, List\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.base import ClassifierMixin\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Optional progress support\n",
    "try:\n",
    "    from tqdm.autonotebook import tqdm\n",
    "    from tqdm_joblib import tqdm_joblib\n",
    "    _HAS_TQDM = True\n",
    "except Exception:\n",
    "    _HAS_TQDM = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2d3616",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d352ce83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1) Data loading\n",
    "\n",
    "def load_fcc_dataset(folder_path: str) -> Tuple[np.ndarray, np.ndarray, List[str], Dict[int, str]]:\n",
    "    \"\"\"\n",
    "    Load the alarm-flood benchmark dataset from a given root folder.\n",
    "\n",
    "    Structure expected:\n",
    "        folder_path/\n",
    "            classA/\n",
    "                file1.csv\n",
    "                file2.csv\n",
    "                ...\n",
    "            classB/\n",
    "                file1.csv\n",
    "                ...\n",
    "\n",
    "    Each CSV is read with pandas, values are transposed (like in user's original code),\n",
    "    and appended as one sample. Labels are assigned according to the sorted subfolder order.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : np.ndarray, shape (n_samples, n_features, ?)\n",
    "        Stacks each CSV (transposed) along axis=0. If CSVs have varying lengths/features,\n",
    "        this will raise; ensure consistency across files/versions.\n",
    "    y : np.ndarray, shape (n_samples,)\n",
    "        Integer labels (0..n_classes-1) aligned with sorted subfolder names.\n",
    "    files : List[str]\n",
    "        List of file paths in the order they were added to X/y.\n",
    "    label_map : Dict[int, str]\n",
    "        Mapping from integer label -> class (subfolder) name for reference.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "\n",
    "    data = []\n",
    "    labels = []\n",
    "    files = []\n",
    "\n",
    "    subfolders = [d for d in sorted(os.listdir(folder_path)) if os.path.isdir(os.path.join(folder_path, d))]\n",
    "    if not subfolders:\n",
    "        raise ValueError(f\"No class subfolders found under: {folder_path}\")\n",
    "\n",
    "    for l, sub in enumerate(subfolders):\n",
    "        sub_path = os.path.join(folder_path, sub)\n",
    "        for fname in sorted(os.listdir(sub_path)):\n",
    "            if fname.endswith(\".csv\"):\n",
    "                fpath = os.path.join(sub_path, fname)\n",
    "                df = pd.read_csv(fpath)\n",
    "                data.append(df.values.transpose())\n",
    "                labels.append(l)\n",
    "                files.append(fpath)\n",
    "\n",
    "    if not data:\n",
    "        raise ValueError(f\"No CSV files found under class subfolders in: {folder_path}\")\n",
    "\n",
    "    # Convert to array (will fail if shapes differ -> enforce consistent preprocessing upstream)\n",
    "    X = np.array(data, dtype=float)\n",
    "    y = np.array(labels, dtype=int)\n",
    "    label_map = {i: sub for i, sub in enumerate(subfolders)}\n",
    "\n",
    "    print(f\"[load_fcc_dataset] Loaded from: {folder_path}\")\n",
    "    # print(f\"  Data shape:  {X.shape}\")\n",
    "    # print(f\"  Labels shape:{y.shape}\")\n",
    "    # print(f\"  Classes:     {label_map}\")\n",
    "\n",
    "    return X, y, files, label_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765fd313",
   "metadata": {},
   "source": [
    "### Data Splits and Training/Testing Suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5f9777c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2) Single-split train/test training + optional model return\n",
    "\n",
    "def _predict_labels_generic(\n",
    "    clf: ClassifierMixin, X: np.ndarray, use_argmin: bool = False\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Predict labels allowing the argmin/argmax option on probabilities (if available).\n",
    "    Fallbacks:\n",
    "      - decision_function -> argmax\n",
    "      - predict -> direct\n",
    "    \"\"\"\n",
    "    if hasattr(clf, \"predict_proba\"):\n",
    "        proba = clf.predict_proba(X)\n",
    "        if use_argmin:\n",
    "            return np.argmin(proba, axis=1)\n",
    "        return np.argmax(proba, axis=1)\n",
    "    elif hasattr(clf, \"decision_function\"):\n",
    "        scores = clf.decision_function(X)\n",
    "        # Ensure 2D\n",
    "        if scores.ndim == 1:  # binary case\n",
    "            scores = np.vstack([-scores, scores]).T\n",
    "        return np.argmax(scores, axis=1)\n",
    "    else:\n",
    "        return clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dfdad12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_once(\n",
    "    classifier: ClassifierMixin,\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 42,\n",
    "    use_argmin: bool = False,\n",
    "    return_model: bool = False,\n",
    "    predefined_indices: Optional[Tuple[np.ndarray, np.ndarray]] = None,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Train a classifier once on a stratified split and evaluate on the test set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    classifier : ClassifierMixin\n",
    "        Any sklearn-compatible classifier.\n",
    "    X, y : np.ndarray\n",
    "        Data and labels.\n",
    "    test_size : float\n",
    "        Fraction for test split.\n",
    "    random_state : int\n",
    "        Seed for reproducibility.\n",
    "    use_argmin : bool\n",
    "        If True and predict_proba available, uses argmin instead of argmax for label selection.\n",
    "    return_model : bool\n",
    "        If True, the trained model is included in the output dict under 'model'.\n",
    "    predefined_indices : (train_idx, test_idx) or None\n",
    "        If provided, use these indices instead of creating a new split.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result : dict\n",
    "        {\n",
    "          'accuracy': float,\n",
    "          'y_true': np.ndarray,\n",
    "          'y_pred': np.ndarray,\n",
    "          'train_idx': np.ndarray,\n",
    "          'test_idx': np.ndarray,\n",
    "          'model': fitted model (if return_model=True)\n",
    "        }\n",
    "    \"\"\"\n",
    "    if predefined_indices is None:\n",
    "        train_idx, test_idx = train_test_split(\n",
    "            np.arange(len(y)),\n",
    "            test_size=test_size,\n",
    "            random_state=random_state,\n",
    "            stratify=y\n",
    "        )\n",
    "    else:\n",
    "        train_idx, test_idx = predefined_indices\n",
    "\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = _predict_labels_generic(classifier, X_test, use_argmin=use_argmin)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    out = {\n",
    "        \"accuracy\": acc,\n",
    "        \"y_true\": y_test,\n",
    "        \"y_pred\": y_pred,\n",
    "        \"train_idx\": train_idx,\n",
    "        \"test_idx\": test_idx,\n",
    "    }\n",
    "    if return_model:\n",
    "        out[\"model\"] = classifier\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a16656",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gianl\\AppData\\Local\\Temp\\ipykernel_16044\\406544817.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "# ---------- helper: dataset distance ----------\n",
    "def _dataset_distance(X_ref: np.ndarray, X_new: np.ndarray) -> float:\n",
    "    diff = X_new - X_ref\n",
    "    nonzero = np.count_nonzero(diff)\n",
    "    total = diff.size\n",
    "    return float(nonzero) / float(total) if total > 0 else 0.0\n",
    "\n",
    "# ---------- helper: two-level traversal ----------\n",
    "def _discover_two_level_versions(versions_root: str) -> list[tuple[str, str]]:\n",
    "    \"\"\"Returns list of (version_name, version_path) with version_name='chance_xx/change_yy'.\"\"\"\n",
    "    version_dirs = []\n",
    "    for lvl1 in sorted(os.listdir(versions_root)):\n",
    "        lvl1_path = os.path.join(versions_root, lvl1)\n",
    "        if not os.path.isdir(lvl1_path):\n",
    "            continue\n",
    "        for lvl2 in sorted(os.listdir(lvl1_path)):\n",
    "            lvl2_path = os.path.join(lvl1_path, lvl2)\n",
    "            if not os.path.isdir(lvl2_path):\n",
    "                continue\n",
    "            version_dirs.append((f\"{lvl1}/{lvl2}\", lvl2_path))\n",
    "    if not version_dirs:\n",
    "        raise ValueError(f\"No two-level version folders found under: {versions_root}\")\n",
    "    return version_dirs\n",
    "\n",
    "# ---------- main: CV experiment with train/val/test (parallelize VERSIONS within each fold) ----------\n",
    "def run_experiment_cv(\n",
    "    benchmark_folder: str,\n",
    "    versions_root: str,\n",
    "    model: ClassifierMixin,\n",
    "    model_params: Any,\n",
    "    *,\n",
    "    time_slice: Optional[int] = 60,\n",
    "    n_splits: int = 5,\n",
    "    val_size: float = 0.2,   # fraction of TRAIN+VAL used as VAL within each fold\n",
    "    random_state: int = 42,\n",
    "    use_argmin_benchmark: bool = True,\n",
    "    use_argmin_eval: bool = True,\n",
    "    enforce_shape_match: bool = True,\n",
    "    n_jobs: int = -1,        # NOW used to parallelize versions per fold\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, Dict[str, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    5-fold CV on the benchmark dataset with train/val/test per fold.\n",
    "    For each version dataset, reuses the exact indices to evaluate:\n",
    "      - benchmark-trained model on val & test,\n",
    "      - alt model trained on version's train, evaluated on val & test.\n",
    "    Returns per-fold results and fold-averaged summary.\n",
    "    \"\"\"\n",
    "    # 1) Load & optionally time-slice BENCHMARK dataset\n",
    "    X_bench, y_bench, _, _ = load_fcc_dataset(benchmark_folder)\n",
    "    if time_slice is not None:\n",
    "        X_bench_used = X_bench[:, :, :time_slice]\n",
    "    else:\n",
    "        X_bench_used = X_bench\n",
    "    print(f\"[Info] Benchmark loaded: X={X_bench.shape}, sliced={X_bench_used.shape}, y={y_bench.shape}\")\n",
    "\n",
    "    # 2) Prepare version discovery and shape checks\n",
    "    versions = _discover_two_level_versions(versions_root)\n",
    "    total_samples = len(y_bench)\n",
    "    print(f\"[Info] Discovered {len(versions)} version(s) under '{versions_root}'\")\n",
    "\n",
    "    # ensure versions match the benchmark shape if requested\n",
    "    def _load_version_or_skip(vname: str, vpath: str):\n",
    "        try:\n",
    "            Xv, yv, _, _ = load_fcc_dataset(vpath)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Skipping version '{vname}' due to load error: {e}\")\n",
    "            return None\n",
    "        if Xv.shape[0] != total_samples:\n",
    "            msg = (f\"[WARN] Version '{vname}' has #samples {Xv.shape[0]} \"\n",
    "                   f\"!= benchmark {total_samples}.\")\n",
    "            if enforce_shape_match:\n",
    "                print(msg + \" Skipping.\")\n",
    "                return None\n",
    "            else:\n",
    "                print(msg + \" Continuing.\")\n",
    "        if time_slice is not None:\n",
    "            Xv = Xv[:, :, :time_slice]\n",
    "        # shape full check (including time dimension)\n",
    "        if Xv.shape != X_bench_used.shape:\n",
    "            msg = (f\"[WARN] Version '{vname}' shape {Xv.shape} \"\n",
    "                   f\"!= benchmark slice shape {X_bench_used.shape}.\")\n",
    "            if enforce_shape_match:\n",
    "                print(msg + \" Skipping.\")\n",
    "                return None\n",
    "            else:\n",
    "                print(msg + \" Continuing.\")\n",
    "        return Xv, yv\n",
    "\n",
    "    # 3) CV splitting (folds stay sequential; versions inside each fold are parallelized)\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    fold_splits = list(enumerate(skf.split(np.arange(total_samples), y_bench)))\n",
    "    print(f\"[Info] Prepared {n_splits}-fold stratified CV\")\n",
    "\n",
    "    fold_entries: List[Dict[str, Any]] = []\n",
    "    indices_meta: Dict[str, np.ndarray] = {}\n",
    "\n",
    "    # --- worker for a single VERSION inside a given fold ---\n",
    "    def _process_version(\n",
    "        fold_id: int,\n",
    "        version_name: str,\n",
    "        version_path: str,\n",
    "        tr_idx_local: np.ndarray,\n",
    "        val_idx_local: np.ndarray,\n",
    "        test_idx: np.ndarray,\n",
    "        bench_model_fold: Any,   # fitted on benchmark train of this fold\n",
    "    ):\n",
    "        out_entries: List[Dict[str, Any]] = []\n",
    "        log_line: str = \"\"\n",
    "        loaded = _load_version_or_skip(version_name, version_path)\n",
    "        if loaded is None:\n",
    "            log_line = f\"[Fold {fold_id}] Skipping version '{version_name}' due to load/shape issues.\"\n",
    "            return out_entries, log_line  # skip silently; warnings already printed\n",
    "        Xv, yv = loaded\n",
    "\n",
    "        # Compute distance once per version (global arrays)\n",
    "        ds_distance = _dataset_distance(X_bench_used, Xv)\n",
    "\n",
    "        # Slice version with SAME indices as the fold\n",
    "        Xtr_v, ytr_v = Xv[tr_idx_local],  yv[tr_idx_local]\n",
    "        Xva_v, yva_v = Xv[val_idx_local], yv[val_idx_local]\n",
    "        Xte_v, yte_v = Xv[test_idx],      yv[test_idx]\n",
    "\n",
    "        acc_b_val = acc_b_test = acc_a_val = acc_a_test = np.nan\n",
    "        # (A) Evaluate BENCHMARK model (trained on benchmark train) on version val/test\n",
    "        for split_name, X_, y_ in ((\"val\", Xva_v, yva_v), (\"test\", Xte_v, yte_v)):\n",
    "            y_pred_b = _predict_labels_generic(bench_model_fold, X_, use_argmin=use_argmin_eval)\n",
    "            acc_b = (y_pred_b == y_).mean()\n",
    "            out_entries.append({\n",
    "                \"fold\": fold_id,\n",
    "                \"dataset_version\": version_name,\n",
    "                \"evaluation\": \"benchmark_model\",\n",
    "                \"split\": split_name,\n",
    "                \"accuracy\": float(acc_b),\n",
    "                \"dataset_distance_to_benchmark\": float(ds_distance),\n",
    "            })\n",
    "            if split_name == \"val\":\n",
    "                acc_b_val = acc_b\n",
    "            elif split_name == \"test\":\n",
    "                acc_b_test = acc_b\n",
    "\n",
    "        # (B) Train ALT model on version train, evaluate on version val/test\n",
    "        alt_clf = model(params=model_params)\n",
    "        alt_clf.fit(Xtr_v, ytr_v)\n",
    "        for split_name, X_, y_ in ((\"val\", Xva_v, yva_v), (\"test\", Xte_v, yte_v)):\n",
    "            y_pred_a = _predict_labels_generic(alt_clf, X_, use_argmin=use_argmin_eval)\n",
    "            acc_a = (y_pred_a == y_).mean()\n",
    "            out_entries.append({\n",
    "                \"fold\": fold_id,\n",
    "                \"dataset_version\": version_name,\n",
    "                \"evaluation\": \"alt_model\",\n",
    "                \"split\": split_name,\n",
    "                \"accuracy\": float(acc_a),\n",
    "                \"dataset_distance_to_benchmark\": float(ds_distance),\n",
    "            })\n",
    "            if split_name == \"val\":\n",
    "                acc_a_val = acc_a\n",
    "            elif split_name == \"test\":\n",
    "                acc_a_test = acc_a\n",
    "\n",
    "        log_line = (\n",
    "            f\"[Fold {fold_id}] Version '{version_name}': \"\n",
    "            f\"bench_val_acc={float(acc_b_val):.4f}, bench_test_acc={float(acc_b_test):.4f}, \"\n",
    "            f\"alt_val_acc={float(acc_a_val):.4f}, alt_test_acc={float(acc_a_test):.4f}, \"\n",
    "            f\"ds_distance={float(ds_distance):.4f}\"\n",
    "        )\n",
    "        return out_entries, log_line\n",
    "\n",
    "    # --- process folds sequentially; parallelize versions within each fold ---\n",
    "    for fold_id, (trainval_idx, test_idx) in fold_splits:\n",
    "        print(f\"[Info] Processing fold {fold_id+1}/{n_splits} …\")\n",
    "\n",
    "        # Stratified VAL split inside the train+val pool\n",
    "        tv_y = y_bench[trainval_idx]\n",
    "        tr_idx_local, val_idx_local = train_test_split(\n",
    "            trainval_idx,\n",
    "            test_size=val_size,\n",
    "            random_state=random_state + fold_id,  # vary seed per fold\n",
    "            stratify=tv_y\n",
    "        )\n",
    "        # Benchmark train/val/test sets\n",
    "        Xtr_b, ytr_b = X_bench_used[tr_idx_local],  y_bench[tr_idx_local]\n",
    "        Xva_b, yva_b = X_bench_used[val_idx_local], y_bench[val_idx_local]\n",
    "        Xte_b, yte_b = X_bench_used[test_idx],      y_bench[test_idx]\n",
    "\n",
    "        # Train a fresh BENCHMARK model on the benchmark train split (per fold)\n",
    "        bench_model_fold = model(params=model_params)\n",
    "        bench_model_fold.fit(Xtr_b, ytr_b)\n",
    "\n",
    "        # Evaluate benchmark model on benchmark val/test (kept as-is)\n",
    "        for split_name, X_, y_ in ((\"val\", Xva_b, yva_b), (\"test\", Xte_b, yte_b)):\n",
    "            y_pred = _predict_labels_generic(bench_model_fold, X_, use_argmin=use_argmin_benchmark)\n",
    "            acc = (y_pred == y_).mean()\n",
    "            fold_entries.append({\n",
    "                \"fold\": fold_id, \"dataset_version\": \"__benchmark__\",\n",
    "                \"evaluation\": \"benchmark_model\", \"split\": split_name,\n",
    "                \"accuracy\": acc, \"dataset_distance_to_benchmark\": 0.0\n",
    "            })\n",
    "\n",
    "        # Cache indices for reuse (same as before)\n",
    "        indices_meta[f\"fold_{fold_id}_train_idx\"] = tr_idx_local\n",
    "        indices_meta[f\"fold_{fold_id}_val_idx\"]   = val_idx_local\n",
    "        indices_meta[f\"fold_{fold_id}_test_idx\"]  = test_idx\n",
    "\n",
    "        # Parallelize the VERSIONS within this fold\n",
    "        version_iterable = versions\n",
    "        if _HAS_TQDM:\n",
    "            print(f\"[Info]  └─ Evaluating {len(versions)} version(s) in parallel …\")\n",
    "            with tqdm_joblib(tqdm(total=len(versions), desc=f\"Fold {fold_id} versions\", leave=True)):\n",
    "                results = Parallel(n_jobs=n_jobs, prefer=\"processes\")(\n",
    "                    delayed(_process_version)(\n",
    "                        fold_id, vname, vpath, tr_idx_local, val_idx_local, test_idx, bench_model_fold\n",
    "                    )\n",
    "                    for (vname, vpath) in version_iterable\n",
    "                )\n",
    "        else:\n",
    "            print(f\"[Info]  └─ Evaluating {len(versions)} version(s) in parallel (tqdm not available) …\")\n",
    "            results = Parallel(n_jobs=n_jobs, prefer=\"processes\")(\n",
    "                delayed(_process_version)(\n",
    "                    fold_id, vname, vpath, tr_idx_local, val_idx_local, test_idx, bench_model_fold\n",
    "                )\n",
    "                for (vname, vpath) in version_iterable\n",
    "            )\n",
    "\n",
    "        # Collect per-version results for this fold\n",
    "        for out_entries, log_line in results:\n",
    "            fold_entries.extend(out_entries)\n",
    "            if log_line:\n",
    "                pass\n",
    "                # print(log_line)\n",
    "\n",
    "    # Build per-fold results (unchanged)\n",
    "    fold_results = pd.DataFrame(fold_entries).sort_values(\n",
    "        [\"dataset_version\", \"evaluation\", \"split\", \"fold\"]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    # Aggregate across folds (unchanged)\n",
    "    summary = (\n",
    "        fold_results\n",
    "        .groupby([\"dataset_version\", \"evaluation\", \"split\"], as_index=False)\n",
    "        .agg(accuracy_mean=(\"accuracy\", \"mean\"),\n",
    "             accuracy_std=(\"accuracy\", \"std\"),\n",
    "             dataset_distance_to_benchmark=(\"dataset_distance_to_benchmark\", \"first\"))\n",
    "        .sort_values([\"dataset_version\", \"evaluation\", \"split\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    print(\"[Info] Completed. Rows -> fold_results:\", len(fold_results), \" | summary:\", len(summary))\n",
    "    return fold_results, summary, indices_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8322935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    split: str = \"test\",   # 'val' or 'test'\n",
    "    save_csv_path: Optional[str] = None,\n",
    "):\n",
    "    if df is None or df.empty:\n",
    "        print(\"No results to plot.\")\n",
    "        return\n",
    "\n",
    "    # detect per-fold vs summary\n",
    "    if \"fold\" in df.columns and \"accuracy\" in df.columns:\n",
    "        # average per version/evaluation for the chosen split\n",
    "        plot_df = (\n",
    "            df[df[\"split\"] == split]\n",
    "            .groupby([\"dataset_version\", \"evaluation\"], as_index=False)\n",
    "            .agg(\n",
    "                accuracy_mean=(\"accuracy\", \"mean\"),\n",
    "                dataset_distance_to_benchmark=(\"dataset_distance_to_benchmark\", \"first\"),\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        plot_df = df[df[\"split\"] == split].rename(columns={\"accuracy_mean\": \"accuracy_mean\"})\n",
    "\n",
    "    df_bench = plot_df[plot_df[\"evaluation\"] == \"benchmark_model\"]\n",
    "    df_alt   = plot_df[plot_df[\"evaluation\"] == \"alt_model\"]\n",
    "\n",
    "    if df_bench.empty and df_alt.empty:\n",
    "        print(f\"No data to plot for split='{split}'.\")\n",
    "        return\n",
    "\n",
    "    df.to_csv(save_csv_path, index=False)\n",
    "    print(f\"Saved results to: {save_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b84bedb",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39387923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifiers\n",
    "from classifiers.WDI_1NN import WDI_1NN\n",
    "from classifiers.ACM_SVM import ACM_SVM\n",
    "from classifiers.CASIM import CASIM\n",
    "from classifiers.EAC_1NN import EAC_1NN\n",
    "from classifiers.MBW_LR import MBW_LR\n",
    "\n",
    "# ---------------- Params \n",
    "wdi_1nn_params = {\"template_threshold\": 0.5}\n",
    "acm_svm_params = None\n",
    "casim_params = {\n",
    "    \"num_features\": 672,\n",
    "    \"n_estimators\": 1,\n",
    "    \"n_jobs_multirocket\": 1,\n",
    "    \"random_state\": 42,\n",
    "    \"alphas\": np.logspace(-3, 3, 10),\n",
    "}\n",
    "eac_1nn_params = {\"attenuation_coefficient_per_min\": 0.001}  # 0.0667}\n",
    "mbw_lr_params = {\n",
    "    \"penalty\": None,\n",
    "    \"fit_intercept\": False,\n",
    "    \"solver\": \"lbfgs\",\n",
    "    \"multi_class\": \"ovr\",\n",
    "    \"decision_bounds\": True,\n",
    "    \"confidence_interval\": 1.96,\n",
    "}\n",
    "\n",
    "# list of models and model_params to evaluate\n",
    "models_to_evaluate = [\n",
    "    (WDI_1NN, wdi_1nn_params, \"WDI-1NN\", True),\n",
    "    (CASIM, casim_params, \"CASIM\", False),\n",
    "    (EAC_1NN, eac_1nn_params, \"EAC-1NN\", True),\n",
    "    (MBW_LR, mbw_lr_params, \"MBW-LR\", False),\n",
    "    (ACM_SVM, acm_svm_params, \"ACM-SVM\", False),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42115b6",
   "metadata": {},
   "source": [
    "#### FCC Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea94192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over models\n",
    "for model, model_params, model_name, use_argmin in models_to_evaluate:\n",
    "    print(f\"\\n=== Running experiment for model: {model_name} ===\")\n",
    "    fold_results, summary, indices_meta = run_experiment_cv(\n",
    "        benchmark_folder=\"data/fcc/benchmark\",\n",
    "        versions_root=\"data/fcc/perturbed_data\",\n",
    "        model=model,\n",
    "        model_params=model_params,\n",
    "        time_slice=60,\n",
    "        n_splits=5,\n",
    "        val_size=0.2,\n",
    "        random_state=42,\n",
    "        use_argmin_benchmark=use_argmin,\n",
    "        use_argmin_eval=use_argmin,\n",
    "        enforce_shape_match=True,\n",
    "    )\n",
    "    # save results\n",
    "    save_results(fold_results, split=\"test\",\n",
    "                 save_csv_path=f\"results/fcc_results_{model_name.lower().replace('-', '_')}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33aac0ad",
   "metadata": {},
   "source": [
    "#### TEP Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2697c453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over models\n",
    "for model, model_params, model_name, use_argmin in models_to_evaluate:\n",
    "    print(f\"\\n=== Running experiment for model: {model_name} ===\")\n",
    "    fold_results, summary, indices_meta = run_experiment_cv(\n",
    "        benchmark_folder=\"data/tep/benchmark\",\n",
    "        versions_root=\"data/tep/perturbed_data\",\n",
    "        model=model,\n",
    "        model_params=model_params,\n",
    "        time_slice=60,\n",
    "        n_splits=5,\n",
    "        val_size=0.2,\n",
    "        random_state=42,\n",
    "        use_argmin_benchmark=use_argmin,\n",
    "        use_argmin_eval=use_argmin,\n",
    "        enforce_shape_match=True,\n",
    "    )\n",
    "    # save results\n",
    "    save_results(fold_results, split=\"test\",\n",
    "                 save_csv_path=f\"results/tep_results_{model_name.lower().replace('-', '_')}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
