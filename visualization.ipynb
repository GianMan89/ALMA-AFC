{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7717c9bd",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbc439d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a885ffa7",
   "metadata": {},
   "source": [
    "### Evaluation and Plotting Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a912ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_selection_strategy(csv_path: str, plot: bool = True):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    csv_path : str\n",
    "        Path to results CSV with columns:\n",
    "        ['dataset_version','evaluation','split','accuracy','accuracy_std','dataset_distance_to_benchmark']\n",
    "    plot : bool\n",
    "        If True, plots a bar chart of average accuracies.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    summary : dict\n",
    "        {\n",
    "          'baseline_val_accuracy': float,\n",
    "          'avg_selection_strategy': float,\n",
    "          'avg_benchmark_only': float,\n",
    "          'avg_alt_only': float,\n",
    "          'num_versions': int,\n",
    "          'num_selected_benchmark': int,\n",
    "          'num_selected_alt': int\n",
    "        }\n",
    "    per_version : pd.DataFrame\n",
    "        One row per dataset_version (excluding '__benchmark__'),\n",
    "        with columns: ['dataset_version','bench_val','bench_test','alt_test','chosen','chosen_test_acc']\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # 1) Baseline: benchmark model on __benchmark__ validation\n",
    "    baseline_rows = df[\n",
    "        (df[\"dataset_version\"] == \"__benchmark__\") &\n",
    "        (df[\"evaluation\"] == \"benchmark_model\") &\n",
    "        (df[\"split\"] == \"val\")\n",
    "    ]\n",
    "    if baseline_rows.empty:\n",
    "        raise ValueError(\"Baseline row not found: __benchmark__/benchmark_model/val\")\n",
    "    baseline = float(baseline_rows[\"accuracy\"].mean())\n",
    "\n",
    "    # 2) Build a per-version table (exclude the __benchmark__ row)\n",
    "    versions = sorted(v for v in df[\"dataset_version\"].unique() if v != \"__benchmark__\")\n",
    "\n",
    "    rows = []\n",
    "    missing = []\n",
    "\n",
    "    # Counters for requested triggers\n",
    "    count_bench_ge_baseline = 0  # times benchmark val on new dataset >= baseline val\n",
    "    count_bench_worse_but_alt_even_worse = 0  # times bench worse than baseline but chosen since alt val < bench val\n",
    "\n",
    "    for v in versions:\n",
    "        # benchmark val & test\n",
    "        bench_val_rows = df[(df[\"dataset_version\"] == v) &\n",
    "                            (df[\"evaluation\"] == \"benchmark_model\") &\n",
    "                            (df[\"split\"] == \"val\")]\n",
    "        bench_test_rows = df[(df[\"dataset_version\"] == v) &\n",
    "                             (df[\"evaluation\"] == \"benchmark_model\") &\n",
    "                             (df[\"split\"] == \"test\")]\n",
    "        # alt val & test (need alt val for the new decision rule)\n",
    "        alt_val_rows = df[(df[\"dataset_version\"] == v) &\n",
    "                          (df[\"evaluation\"] == \"alt_model\") &\n",
    "                          (df[\"split\"] == \"val\")]\n",
    "        alt_test_rows = df[(df[\"dataset_version\"] == v) &\n",
    "                           (df[\"evaluation\"] == \"alt_model\") &\n",
    "                           (df[\"split\"] == \"test\")]\n",
    "\n",
    "        if bench_val_rows.empty or bench_test_rows.empty or alt_test_rows.empty or alt_val_rows.empty:\n",
    "            missing.append(v)\n",
    "            continue\n",
    "\n",
    "        bench_val  = float(bench_val_rows[\"accuracy\"].mean())\n",
    "        bench_test = float(bench_test_rows[\"accuracy\"].mean())\n",
    "        alt_val    = float(alt_val_rows[\"accuracy\"].mean())\n",
    "        alt_test   = float(alt_test_rows[\"accuracy\"].mean())\n",
    "\n",
    "        # Selection strategy:\n",
    "        # 1) If benchmark VAL on new dataset >= baseline -> choose benchmark TEST\n",
    "        # 2) Else (benchmark worse than baseline), compare alt VAL vs benchmark VAL on the new dataset:\n",
    "        #       - if alt VAL >= bench VAL -> choose alt TEST\n",
    "        #       - else -> choose benchmark TEST (count this special trigger)\n",
    "        if bench_val >= baseline:\n",
    "            chosen = \"benchmark_model\"\n",
    "            chosen_acc = bench_test\n",
    "            count_bench_ge_baseline += 1\n",
    "        else:\n",
    "            if alt_val >= bench_val:\n",
    "                chosen = \"alt_model\"\n",
    "                chosen_acc = alt_test\n",
    "            else:\n",
    "                chosen = \"benchmark_model\"\n",
    "                chosen_acc = bench_test\n",
    "                count_bench_worse_but_alt_even_worse += 1\n",
    "\n",
    "        rows.append({\n",
    "            \"dataset_version\": v,\n",
    "            \"bench_val\": bench_val,\n",
    "            \"bench_test\": bench_test,\n",
    "            \"alt_test\": alt_test,\n",
    "            \"chosen\": chosen,\n",
    "            \"chosen_test_acc\": chosen_acc,\n",
    "        })\n",
    "\n",
    "    per_version = pd.DataFrame(rows).sort_values(\"dataset_version\").reset_index(drop=True)\n",
    "\n",
    "    if not per_version.empty:\n",
    "        avg_selection = float(per_version[\"chosen_test_acc\"].mean())\n",
    "        avg_bench_only = float(per_version[\"bench_test\"].mean())\n",
    "        avg_alt_only = float(per_version[\"alt_test\"].mean())\n",
    "        n_sel_bench = int((per_version[\"chosen\"] == \"benchmark_model\").sum())\n",
    "        n_sel_alt = int((per_version[\"chosen\"] == \"alt_model\").sum())\n",
    "    else:\n",
    "        avg_selection = avg_bench_only = avg_alt_only = float(\"nan\")\n",
    "        n_sel_bench = n_sel_alt = 0\n",
    "\n",
    "    # 3) Print results\n",
    "    print(f\"Baseline (benchmark __benchmark__ VAL): {baseline:.4f}\")\n",
    "    print(f\"Versions included: {len(per_version)}\")\n",
    "    if missing:\n",
    "        print(f\"[WARN] Skipped {len(missing)} versions due to missing rows: {missing[:5]}{' ...' if len(missing)>5 else ''}\")\n",
    "    print(f\"Selected benchmark for {n_sel_bench} versions; alt for {n_sel_alt} versions.\")\n",
    "    print(f\"Trigger counts:\")\n",
    "    print(f\"  • Benchmark ≥ baseline on new VAL (picked benchmark): {count_bench_ge_baseline}\")\n",
    "    print(f\"  • Benchmark < baseline on new VAL but alt VAL worse (still picked benchmark): {count_bench_worse_but_alt_even_worse}\")\n",
    "    print(f\"\\nAverage TEST accuracy (selection strategy): {avg_selection:.4f}\")\n",
    "    print(f\"Average TEST accuracy (benchmark-only):     {avg_bench_only:.4f}\")\n",
    "    print(f\"Average TEST accuracy (alt-only):           {avg_alt_only:.4f}\")\n",
    "\n",
    "    # 4) Plot\n",
    "    if plot:\n",
    "        labels = [\"Selection strategy\", \"Benchmark-only\", \"Alt-only\"]\n",
    "        values = [avg_selection, avg_bench_only, avg_alt_only]\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.bar(labels, values)\n",
    "        plt.ylabel(\"Average TEST accuracy\")\n",
    "        plt.title(\"Average Test Accuracy by Strategy\")\n",
    "        for i, v in enumerate(values):\n",
    "            plt.text(i, v + 0.002, f\"{v:.3f}\", ha=\"center\", va=\"bottom\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    summary = {\n",
    "        \"baseline_val_accuracy\": baseline,\n",
    "        \"avg_selection_strategy\": avg_selection,\n",
    "        \"avg_benchmark_only\": avg_bench_only,\n",
    "        \"avg_alt_only\": avg_alt_only,\n",
    "        \"num_versions\": len(per_version),\n",
    "        \"num_selected_benchmark\": n_sel_bench,\n",
    "        \"num_selected_alt\": n_sel_alt,\n",
    "        \"num_bench_ge_baseline_val\": count_bench_ge_baseline,\n",
    "        \"num_bench_worse_but_alt_even_worse\": count_bench_worse_but_alt_even_worse,\n",
    "    }\n",
    "    return summary, per_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed4210d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    split: str = \"test\",   # 'val' or 'test'\n",
    "    title: str = \"Performance vs. Distance to Benchmark Dataset\",\n",
    "    legend_benchmark: str = \"Benchmark model\",\n",
    "    legend_alt: str = \"Alt model\",\n",
    "    save_csv_path: Optional[str] = None,\n",
    "    show: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot accuracy (mean over folds if per-fold data is provided) vs dataset distance for a given split.\n",
    "    Accepts either:\n",
    "      - per-fold results with columns ['fold','evaluation','split','dataset_distance_to_benchmark','accuracy'], or\n",
    "      - summary with ['evaluation','split','dataset_distance_to_benchmark','accuracy_mean'].\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    if df is None or df.empty:\n",
    "        print(\"No results to plot.\")\n",
    "        return\n",
    "\n",
    "    # detect per-fold vs summary\n",
    "    if \"fold\" in df.columns and \"accuracy\" in df.columns:\n",
    "        # average per version/evaluation for the chosen split\n",
    "        plot_df = (\n",
    "            df[df[\"split\"] == split]\n",
    "            .groupby([\"dataset_version\", \"evaluation\"], as_index=False)\n",
    "            .agg(\n",
    "                accuracy_mean=(\"accuracy\", \"mean\"),\n",
    "                dataset_distance_to_benchmark=(\"dataset_distance_to_benchmark\", \"first\"),\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        plot_df = df[df[\"split\"] == split].rename(columns={\"accuracy_mean\": \"accuracy_mean\"})\n",
    "\n",
    "    df_bench = plot_df[plot_df[\"evaluation\"] == \"benchmark_model\"]\n",
    "    df_alt   = plot_df[plot_df[\"evaluation\"] == \"alt_model\"]\n",
    "\n",
    "    if df_bench.empty and df_alt.empty:\n",
    "        print(f\"No data to plot for split='{split}'.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    if not df_bench.empty:\n",
    "        plt.scatter(\n",
    "            df_bench[\"dataset_distance_to_benchmark\"], df_bench[\"accuracy_mean\"],\n",
    "            label=legend_benchmark, marker=\"o\"\n",
    "        )\n",
    "    if not df_alt.empty:\n",
    "        plt.scatter(\n",
    "            df_alt[\"dataset_distance_to_benchmark\"], df_alt[\"accuracy_mean\"],\n",
    "            label=legend_alt, marker=\"^\"\n",
    "        )\n",
    "\n",
    "    plt.xlabel(\"Dataset distance to benchmark (0–1)\")\n",
    "    plt.ylabel(f\"{split.capitalize()} accuracy (mean over folds)\")\n",
    "    plt.title(title + f\" — {split.upper()} split\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "    if save_csv_path:\n",
    "        df.to_csv(save_csv_path, index=False)\n",
    "        print(f\"Saved results to: {save_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ce62e8",
   "metadata": {},
   "source": [
    "### Parametrization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46500eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifiers\n",
    "from classifiers.WDI_1NN import WDI_1NN\n",
    "from classifiers.ACM_SVM import ACM_SVM\n",
    "from classifiers.CASIM import CASIM\n",
    "from classifiers.EAC_1NN import EAC_1NN\n",
    "from classifiers.MBW_LR import MBW_LR\n",
    "\n",
    "# ---------------- Params \n",
    "wdi_1nn_params = {\"template_threshold\": 0.5}\n",
    "acm_svm_params = None\n",
    "casim_params = {\n",
    "    \"num_features\": 672,\n",
    "    \"n_estimators\": 1,\n",
    "    \"n_jobs_multirocket\": 1,\n",
    "    \"random_state\": 42,\n",
    "    \"alphas\": np.logspace(-3, 3, 10),\n",
    "}\n",
    "eac_1nn_params = {\"attenuation_coefficient_per_min\": 0.001}  # 0.0667}\n",
    "mbw_lr_params = {\n",
    "    \"penalty\": None,\n",
    "    \"fit_intercept\": False,\n",
    "    \"solver\": \"lbfgs\",\n",
    "    \"multi_class\": \"ovr\",\n",
    "    \"decision_bounds\": True,\n",
    "    \"confidence_interval\": 1.96,\n",
    "}\n",
    "\n",
    "# list of models and model_params to evaluate\n",
    "models_to_evaluate = [\n",
    "    (WDI_1NN, wdi_1nn_params, \"WDI-1NN\", True),\n",
    "    (CASIM, casim_params, \"CASIM\", False),\n",
    "    (EAC_1NN, eac_1nn_params, \"EAC-1NN\", True),\n",
    "    (MBW_LR, mbw_lr_params, \"MBW-LR\", False),\n",
    "    (ACM_SVM, acm_svm_params, \"ACM-SVM\", False),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61540a4f",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3d9ccb",
   "metadata": {},
   "source": [
    "#### TEP Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38c0643",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"fcc\"]\n",
    "splits = [\"test\"]\n",
    "\n",
    "for dataset in datasets:\n",
    "    for _, _, model_name, _ in models_to_evaluate:\n",
    "        csv_path = f\"results/{dataset}_results_{model_name.lower().replace('-', '_')}.csv\"\n",
    "        if not os.path.isfile(csv_path):\n",
    "            print(f\"[WARN] Results CSV not found: {csv_path}. Skipping.\")\n",
    "            continue\n",
    "        print(f\"\\n[INFO] Loading and plotting: {csv_path}\")\n",
    "        df = pd.read_csv(csv_path)\n",
    "        for split in splits:\n",
    "            try:\n",
    "                plot_results(\n",
    "                    df,\n",
    "                    split=split,\n",
    "                    title=f\"{model_name} — {dataset.upper()} — Distance vs Performance\",\n",
    "                    show=True\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Failed to plot {csv_path} (split={split}): {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e827d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, model_params, model_name, use_argmin in models_to_evaluate:\n",
    "    print(f\"\\n=== Evaluating selection strategy for model: {model_name} ===\")\n",
    "    csv_path = f\"results/fcc_results_{model_name.lower().replace('-', '_')}.csv\"\n",
    "    if not os.path.isfile(csv_path):\n",
    "        print(f\"[WARN] Results CSV not found: {csv_path}. Skipping.\")\n",
    "        continue\n",
    "    summary, per_version = evaluate_selection_strategy(csv_path, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f46bbf5",
   "metadata": {},
   "source": [
    "#### FCC Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b80df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"tep\"]\n",
    "splits = [\"test\"]\n",
    "\n",
    "for dataset in datasets:\n",
    "    for _, _, model_name, _ in models_to_evaluate:\n",
    "        csv_path = f\"results/{dataset}_results_{model_name.lower().replace('-', '_')}.csv\"\n",
    "        if not os.path.isfile(csv_path):\n",
    "            print(f\"[WARN] Results CSV not found: {csv_path}. Skipping.\")\n",
    "            continue\n",
    "        print(f\"\\n[INFO] Loading and plotting: {csv_path}\")\n",
    "        df = pd.read_csv(csv_path)\n",
    "        for split in splits:\n",
    "            try:\n",
    "                plot_results(\n",
    "                    df,\n",
    "                    split=split,\n",
    "                    title=f\"{model_name} — {dataset.upper()} — Distance vs Performance\",\n",
    "                    show=True\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Failed to plot {csv_path} (split={split}): {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c6c798",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, model_params, model_name, use_argmin in models_to_evaluate:\n",
    "    print(f\"\\n=== Evaluating selection strategy for model: {model_name} ===\")\n",
    "    csv_path = f\"results/tep_results_{model_name.lower().replace('-', '_')}.csv\"\n",
    "    if not os.path.isfile(csv_path):\n",
    "        print(f\"[WARN] Results CSV not found: {csv_path}. Skipping.\")\n",
    "        continue\n",
    "    summary, per_version = evaluate_selection_strategy(csv_path, plot=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
